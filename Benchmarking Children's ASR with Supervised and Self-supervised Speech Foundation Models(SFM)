Prerequisites:
ASR - Automatic speech recognition, a system that converts speech to text (ex. siri)
Foundational Models - large trained model on general data (ex. Whisper, Wav2Vec 2.0, HuBERT for ASR)
Speech Foundational Models - SFMs
Supervised learning - training model using labeled data(input with its answer)
self-supervised - used unlabelled data but creates its own label
fine-tuning - training a foundational model on specific data
zero-shot - using a foundational model without fine tuing
pre-trained - model trained on large general data
word-error-rate(WER) - used to measure accuracy of ASR.
data augmentation - generating more training data,by modifying existing training data
parameter efficient fine tuning(PEFT) - fine tuning without updataing all para
speech corpora - set of speech data with audio+transcripts
benchmarking - using same set of data on different models to analysis and compare models
vocabulary - building blocks of text to be generated by model
Objective Function - loss function tells the model how wrong it is
Connectionist Temporal Classification (CTC) - makes text to align with speech by generating sequence of probabilities and blank token between time frame.

Objective of paper:
1. Compare different child ASR models
2. Investigate fine-tuning methods by comparing data augmentation,PEFT . also using PIF(perturbation invariant finetuning) to stabilize models from noisy data generated from data augmentation.

Intro:
Before several SFMs were compared using different speech corpora, making it difficult to compare , 
IN this paper they have used OGI read and MyST spontaneous child speech corpora for different SFM so that it is easy to benchmark.
They have also looked for best fine tuning strategies for differnet SFM with help of data augumentation and PEFT.

Methods in benchmarking:
1. SFM
there are two types: 
     i) Supervised (wishper, parakeet)
     ii) self supervised (Wav2vec2.0, HuBERT, and WavLM)
2. Data Augumentation
Since child sample datasets are limited, they used data augumentation to create more dataset. they used below methods of data augumentions to create new data;
i) Pitch Perturbation- change the pitch
ii) Speed Perturbation - change the speed
iii) Vocal Tract Length Perturbation
iv) SpecAugment - time masking

3. parameter efficient fine tuninge(PEFT)
PEFT is used to minimize the computational resources required for training a large model for a specific task/fine tuning.
It has four techniques:
i)LoRA(lower rank adaptation) - adds a small adjustment layer to the main weight so that we dont need to traing everthing again . only the new layer.
ii) adapter tuning - adds a small layer between each layer and only this newly added layers are trained.
iii) prompt tuning - giving a specific prompt in input allowing model to learn only prompt specific
iv) prefix tunig - similar to prompt tuning , but gives hint at each layer instead of just input

4. Child speech database:
two database were use:
i) MyST corpus — spontaneous (natural) speech from tutoring sessions
ii) CSLU OGI Kids corpus — scripted (reading) speech from children
both datasets were used to train zero-shot models. 
We observed that the larger the size of parameters/model size,lesser the word error rate, better the results.

5. Finetuning and Evaluation Setup
  i) Supervised models - while fine tuning it does not need to change in vocabulary or objective function.
  ii) self supervised - used vocabulary and CTC to avoid loss and 

6. zero shot on supervised SFMs
zero shot performed on supervised models which are used for adults with child data. and models like Canary and Parakeet performed the best. eventhough whisper was trained with more data . we conclude that quality of data>> quantity

7.finetuning on foundational models
Supervised speech models (like Whisper, Canary, and Parakeet) work better for children’s ASR after fine-tuning than self-supervised models (like WavLM).
WavLM is still strong among self-supervised ones because it learned from noisy and multi-speaker data. SSL models are better at understanding general speech features — like tone, pitch, rhythm, and pronunciation patterns

8.Comparisons of Data Augmentation Methods
This section looks at how data augmentation (DA) methods affect the performance of child speech recognition using the Whisper-small model. Because there isn’t much data available for children’s speech, the authors tried several common augmentation techniques — like changing the speed or pitch — to make the model more robust.
They found that all the DA methods gave small but similar improvements after fine-tuning. Combining multiple methods didn’t help much, probably because Whisper was already trained on large and diverse data, so it can handle such variations well.
Some methods, like pitch perturbation and vocal tract length perturbation, gave unstable results. To fix this, the authors introduced a new approach called Perturbation Invariant Finetuning (PIF), which helps the model stay stable by making it treat the original and modified speech as the same.


9.Comparisons of Parameter Efficient Finetuning
When large speech models (like Whisper) are fine-tuned for specific tasks (like child ASR), it usually requires updating billions of parameters, which is very expensive and memory-heavy.
To make this more efficient, researchers use Parameter-Efficient Finetuning (PEFT) — methods that only train a small subset of parameters while keeping the main model frozen.
They tested different PEFT methods using the Whisper-small model on the MyST dataset and compared them to full fine-tuning.PEFT can save huge computation costs, and adapter tuning works the best among all methods tested.
However, careful initialization of adapters (making them start as an “identity” layer) is crucial for good results.

10.Impact of Model size on PEFT performance
Here, the authors studied how the size of the Whisper model (tiny → largeV3) affects PEFT performance — specifically, adapter tuning, since it worked best earlier.They plotted WER (Word Error Rate) for both full finetuning and adapter finetuning across different model sizes.
Use full finetuning for small models
Use PEFT for large models

11.Conclusion
After benchmarking different ASR foundational models we noticed Canary and Parakeet models are better than Whisper models on child speech with much less training data, indicating the data quality is sometimes more important than the data quantity.
We also noticed that Supervised SFMs are better than self-supervised after fine tuning.
also investecated fine tuning stratigies by comparing various data augumentation startegies and PEFT methods.
While using augumentaed data we used PIF(perturbation invariant finetuning) loss which act as stabilizer to avoid traingin on noisy data.
Compared various PEFT strategies and found that behaviou of PEFT changes with increase in model size. Also noticed that PEFT performed better than full finetuning for large models but worse for small models



Resource : https://www.seas.ucla.edu/spapl/paper/Ruchao_IS_2024.pdf



